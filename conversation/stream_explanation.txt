LINE-BY-LINE EXPLANATION: stream.py
==========================================

Line 1: import requests
- Imports the requests library (not used in this code)

Line 2: from langchain_ollama import ChatOllama
- Imports ChatOllama class to interact with Ollama language models

Line 4-7: model = ChatOllama(model="llama3.1:8b", temperature=0.7)
- Creates a ChatOllama instance using llama3.1 8B model with temperature 0.7 (balanced creativity)

Line 8: for chunk in model.stream('hello, what is python ?'):
- Streams the model's response to the question in chunks for real-time output

Line 9: print(chunk.text, end='', flush=True)
- Prints each chunk of text as it arrives without newlines, flushing immediately for real-time display
